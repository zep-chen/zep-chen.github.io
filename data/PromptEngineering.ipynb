{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Base LLM: predicts next word.\n",
    "* Instruction Tuned LLM: follow instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-0.27.6-py3-none-any.whl (71 kB)\n",
      "     ---------------------------------------- 71.9/71.9 kB 3.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\fattree\\anaconda3\\lib\\site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\fattree\\anaconda3\\lib\\site-packages (from openai) (2.28.2)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.4-cp38-cp38-win_amd64.whl (324 kB)\n",
      "     ------------------------------------- 324.5/324.5 kB 10.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\fattree\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fattree\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fattree\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\fattree\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (3.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\fattree\\anaconda3\\lib\\site-packages (from aiohttp->openai) (19.3.0)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp38-cp38-win_amd64.whl (34 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp38-cp38-win_amd64.whl (28 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.2-cp38-cp38-win_amd64.whl (61 kB)\n",
      "     ---------------------------------------- 61.8/61.8 kB 3.2 MB/s eta 0:00:00\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\fattree\\anaconda3\\lib\\site-packages (from tqdm->openai) (0.4.6)\n",
      "Installing collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
      "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.6 yarl-1.9.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-0863633b1d8c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdotenv\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfind_dotenv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfind_dotenv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "openai.api_key  = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principles and Tactics of Prompting\n",
    "1. Write clear and specific instructions.\n",
    "    1. Use delimiters: `Summarize the text delimited by triple backticks into a single sentence.`\n",
    "        * Triple quotes: \"\"\"\n",
    "        * Triple backticks: ```\n",
    "        * Triple dashes: ---\n",
    "        * Angle brackets: <>\n",
    "        * XML tags: `<tag> </tag>`\n",
    "    2. Ask for a structured output, e.g., JSON or HTML. `Generate a list of xx. Provide them in JSON format with the following keys: book_id, title, author, genre.`\n",
    "    3. Ask the model to check whether conditions are satisfied. `If it contains a sequence of instructions, re-write those instructions in the following format: Step 1 - ...; If the text does not contain a sequence of instructions, then simply write \\\"No steps provided.\\\"`\n",
    "    4. \"Few-shot\" prompting: Give successful examples of completing tasks. `Your task is to answer in a consistent style. <Question1>; <Answer1>; <Question2>.`\n",
    "2. Give the model time to think.\n",
    "    1. Specify the steps to complete a task. `Perform the following actions: 1. Summarize the following text. 2. Translate the summary into French. 3. List each name in the French summary. 4. Output a json object that contains the following keys: french_summary, num_names.`\n",
    "    2. Instruct the model to work out its own solution before rushing to a conclusion. `Use the following format: <Question>; <Student's solution>; <Actual solution>; <Is the student's solution the same as actual solution just calculated: yes or no>; <Student grade: correct or incorrect>`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Process\n",
    "* Change raw data into the json format.\n",
    "The text delimited by triple backticks is the data, every row represents the key and the value, and the key and the value are seperated by colon. Do not show me the code. Help me transfer the text to the json format.\n",
    "\n",
    "### Writing\n",
    "* plural and 拼写错误\n",
    "\n",
    "* Find a better expression from Chinese to English. This function can be combined with the zotero search of attachment content.\n",
    "```\n",
    "How should I expressed the text delimited by triple backticks in english? Give me three sentences  as examples. This is a translation task.\n",
    "```\n",
    "* Academic Style:\n",
    "```\n",
    "Rewrite the text delimited by triple backticks in clear and in academic language.\n",
    "```\n",
    "* Grammar Check:\n",
    "```\n",
    "Please make suggestions for fixing any language issues in the text delimited by triple backticks and provide an explanation for each suggested change.\n",
    "```\n",
    "* Report Generation:\n",
    "\n",
    "### Coding\n",
    "* Visualization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the model has a limitation called **hallucinations**, i.e., it may make statements that sound plausible but are not true. One way to reduce hallucinations: `First find relevant information, then answer the question based on the relevant information.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key  = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Prompt Development\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f267ea1fdaa4b9f28fa612f1cc31c207967bebb67c16d84190c7018c8e0a779f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
